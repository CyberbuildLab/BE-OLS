{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from openpyxl import load_workbook\n",
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, Literal, URIRef\n",
    "from rdflib.namespace import DC, DCTERMS, SKOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path_ontologies = filedialog.askopenfilename(title=\"Select file listing all ontologies)\")\n",
    "#file_path_ontologies = \"C:/Users/fbosche/Documents/GitHub/BE-OLS/data/source/ontologies_source.xlsx\"\n",
    "file_path_ontologies = \"C:/Users/fbosche/University College London/EC3 - 1. Modelling and Standards - 1. Modelling and Standards/Material/Project D_Ontologies/Scripts/ontologies_source.xlsx\"\n",
    "print(file_path_ontologies)\n",
    "\n",
    "# Read the 'Data' sheet from the file into a pandas DataFrame\n",
    "sheet_name = 'Data'\n",
    "try:\n",
    "    df_ontologies = pd.read_excel(file_path_ontologies, sheet_name=sheet_name)\n",
    "    print(f\"{sheet_name} data loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading {sheet_name} data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Information from TTL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common ontology namespaces\n",
    "DC = Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "VANN = Namespace(\"http://purl.org/vocab/vann/\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "\n",
    "\n",
    "def normalize_uri(uri: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize URI for comparison:\n",
    "    - Convert https to http\n",
    "    - Remove trailing / or #\n",
    "    \n",
    "    This ensures URIs like 'https://w3id.org/dot#' and 'https://w3id.org/dot/' \n",
    "    will match after normalization.\n",
    "    \n",
    "    Args:\n",
    "        uri: URI string to normalize\n",
    "        \n",
    "    Returns:\n",
    "        Normalized URI string\n",
    "    \"\"\"\n",
    "    if not uri:\n",
    "        return uri\n",
    "    if uri.startswith('https://'):\n",
    "        uri = 'http://' + uri[8:]\n",
    "    return uri.rstrip('/#')\n",
    "\n",
    "\n",
    "def is_prefix_used_in_graph(prefix_uri, all_uris):\n",
    "    \"\"\"\n",
    "    Check if a prefix namespace URI is actually used in the graph.\n",
    "    A prefix is considered used if any URI in the graph starts with the prefix URI.\n",
    "    \n",
    "    Args:\n",
    "        prefix_uri: The namespace URI of the prefix (e.g., 'http://purl.org/dc/terms/')\n",
    "        all_uris: Set of all URIs found in the graph (subjects, predicates, and objects)\n",
    "        \n",
    "    Returns:\n",
    "        True if the prefix is used, False otherwise\n",
    "    \"\"\"\n",
    "    # Normalize the prefix URI (but keep track of original ending)\n",
    "    prefix_ends_with_separator = prefix_uri.endswith('/') or prefix_uri.endswith('#')\n",
    "    normalized_prefix = normalize_uri(prefix_uri)\n",
    "    \n",
    "    for uri in all_uris:\n",
    "        normalized_uri = normalize_uri(uri)\n",
    "        \n",
    "        # Exact match\n",
    "        if normalized_uri == normalized_prefix:\n",
    "            return True\n",
    "        \n",
    "        # If prefix originally ended with / or #, check if URI starts with normalized prefix\n",
    "        if prefix_ends_with_separator:\n",
    "            if normalized_uri.startswith(normalized_prefix + '/') or \\\n",
    "               normalized_uri.startswith(normalized_prefix + '#') or \\\n",
    "               (len(normalized_uri) > len(normalized_prefix) and \n",
    "                normalized_uri.startswith(normalized_prefix)):\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_ontology_uris(ttl_file_path, debug=False):\n",
    "    \"\"\"\n",
    "    Extract ontology URIs with detailed information from a TTL file.\n",
    "    Only includes prefixes that are actually used in the ontology content.\n",
    "    Prevents duplicate URI bases (e.g., when two prefixes point to the same URI).\n",
    "    \n",
    "    Args:\n",
    "        ttl_file_path: Path to the TTL file\n",
    "        debug: If True, print debug information\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts with 'prefix' and 'ontology_base' keys\n",
    "    \"\"\"\n",
    "    # Read file to extract only the prefix declarations actually in the file\n",
    "    prefix_map = {}\n",
    "    with open(ttl_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # Match @prefix declarations\n",
    "            if line.startswith('@prefix'):\n",
    "                match = re.match(r'@prefix\\s+(\\w*):\\s+<(.+?)>', line)\n",
    "                if match:\n",
    "                    prefix_map[match.group(1)] = match.group(2)\n",
    "            # Match PREFIX declarations (SPARQL style)\n",
    "            elif line.upper().startswith('PREFIX'):\n",
    "                match = re.match(r'PREFIX\\s+(\\w*):\\s+<(.+?)>', line, re.IGNORECASE)\n",
    "                if match:\n",
    "                    prefix_map[match.group(1)] = match.group(2)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  Prefixes declared in file: {list(prefix_map.keys())}\")\n",
    "    \n",
    "    g = Graph()\n",
    "    g.parse(ttl_file_path, format='turtle')\n",
    "\n",
    "    # Find the URI of the ontology being analyzed\n",
    "    own_ontology_uris = set()\n",
    "    own_ontology_prefixes = set()\n",
    "    \n",
    "    for s in g.subjects(RDF.type, OWL.Ontology):\n",
    "        uri = str(s)\n",
    "        if uri.startswith('http://') or uri.startswith('https://'):\n",
    "            if '#' in uri:\n",
    "                base = uri.rsplit('#', 1)[0] + '#'\n",
    "            else:\n",
    "                base = uri.rsplit('/', 1)[0] + '/'\n",
    "            own_ontology_uris.add(normalize_uri(base))\n",
    "            own_ontology_uris.add(normalize_uri(uri))\n",
    "            \n",
    "            # Find if this ontology has a prefix\n",
    "            for prefix, prefix_uri in prefix_map.items():\n",
    "                if normalize_uri(prefix_uri) == normalize_uri(uri) or \\\n",
    "                   normalize_uri(prefix_uri) == normalize_uri(base):\n",
    "                    own_ontology_prefixes.add(prefix)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  Own ontology URIs (normalized): {own_ontology_uris}\")\n",
    "        print(f\"  Own ontology prefixes: {own_ontology_prefixes}\")\n",
    "\n",
    "    # Collect ALL full URIs used in subjects, predicates, AND objects\n",
    "    all_uris = set()\n",
    "    uri_bases = set()\n",
    "    \n",
    "    for s, p, o in g:\n",
    "        # Check subject\n",
    "        uri = str(s)\n",
    "        if uri.startswith('http://') or uri.startswith('https://'):\n",
    "            all_uris.add(uri)\n",
    "            if '#' in uri:\n",
    "                base = uri.rsplit('#', 1)[0] + '#'\n",
    "            else:\n",
    "                base = uri.rsplit('/', 1)[0] + '/'\n",
    "            uri_bases.add(base)\n",
    "        \n",
    "        # Check predicate\n",
    "        uri = str(p)\n",
    "        if uri.startswith('http://') or uri.startswith('https://'):\n",
    "            all_uris.add(uri)\n",
    "            if '#' in uri:\n",
    "                base = uri.rsplit('#', 1)[0] + '#'\n",
    "            else:\n",
    "                base = uri.rsplit('/', 1)[0] + '/'\n",
    "            uri_bases.add(base)\n",
    "        \n",
    "        # Check object - but only if it's a URI, not a Literal\n",
    "        if isinstance(o, URIRef):\n",
    "            uri = str(o)\n",
    "            if uri.startswith('http://') or uri.startswith('https://'):\n",
    "                all_uris.add(uri)\n",
    "                if '#' in uri:\n",
    "                    base = uri.rsplit('#', 1)[0] + '#'\n",
    "                else:\n",
    "                    base = uri.rsplit('/', 1)[0] + '/'\n",
    "                uri_bases.add(base)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  Total unique URIs in graph (incl. objects): {len(all_uris)}\")\n",
    "    \n",
    "    # Remove the ontology being analyzed from uri_bases\n",
    "    uri_bases_filtered = set()\n",
    "    for base in uri_bases:\n",
    "        norm_base = normalize_uri(base)\n",
    "        if norm_base not in own_ontology_uris:\n",
    "            uri_bases_filtered.add(base)\n",
    "    uri_bases = uri_bases_filtered\n",
    "    \n",
    "    # Merge prefix mappings and discovered ontology bases\n",
    "    ontology_list = []\n",
    "    added_bases = set()\n",
    "    \n",
    "    # Only add prefix mappings that are ACTUALLY USED in the content\n",
    "    # Also prevent duplicates (e.g., two prefixes pointing to the same URI)\n",
    "    for prefix, prefix_uri in prefix_map.items():\n",
    "        if prefix not in own_ontology_prefixes:\n",
    "            is_used = is_prefix_used_in_graph(prefix_uri, all_uris)\n",
    "            normalized = normalize_uri(prefix_uri)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"  Prefix '{prefix}' ({prefix_uri}): used={is_used}, already_added={normalized in added_bases}\")\n",
    "            \n",
    "            # FIX: Check if this base was already added (handles duplicate prefixes)\n",
    "            if is_used and normalized not in added_bases:\n",
    "                ontology_list.append({\n",
    "                    'prefix_auto': prefix,\n",
    "                    'ontology_base': prefix_uri\n",
    "                })\n",
    "                added_bases.add(normalized)\n",
    "            elif is_used and normalized in added_bases:\n",
    "                if debug:\n",
    "                    print(f\"    -> Skipping duplicate prefix '{prefix}' (URI already added)\")\n",
    "    \n",
    "    # Add any discovered bases that weren't matched to prefixes\n",
    "    for base in sorted(uri_bases):\n",
    "        normalized_base = normalize_uri(base)\n",
    "        if normalized_base not in added_bases:\n",
    "            ontology_list.append({\n",
    "                'prefix_auto': None,\n",
    "                'ontology_base': base\n",
    "            })\n",
    "            added_bases.add(normalized_base)\n",
    "            if debug:\n",
    "                print(f\"  Added base without prefix: {base}\")\n",
    "    \n",
    "    return ontology_list\n",
    "\n",
    "\n",
    "def extract_ontology_metadata(ttl_file_path):\n",
    "    \"\"\"\n",
    "    Extract metadata from the ontology being analyzed.\n",
    "    \n",
    "    Args:\n",
    "        ttl_file_path: Path to the TTL file\n",
    "        \n",
    "    Returns:\n",
    "        dict with ontology metadata fields\n",
    "    \"\"\"\n",
    "    g = Graph()\n",
    "    g.parse(ttl_file_path, format='turtle')\n",
    "    \n",
    "    # Find the ontology URI\n",
    "    ontology_uri = None\n",
    "    for s in g.subjects(RDF.type, OWL.Ontology):\n",
    "        ontology_uri = s\n",
    "        break\n",
    "    \n",
    "    if not ontology_uri:\n",
    "        return {\n",
    "            'error': 'No owl:Ontology declaration found in the file'\n",
    "        }\n",
    "    \n",
    "    def get_value(predicate_list):\n",
    "        \"\"\"Helper to get the first available value from a list of predicates\"\"\"\n",
    "        for pred in predicate_list:\n",
    "            for obj in g.objects(ontology_uri, pred):\n",
    "                if isinstance(obj, Literal):\n",
    "                    return str(obj)\n",
    "                else:\n",
    "                    return str(obj)\n",
    "        return None\n",
    "    \n",
    "    def get_all_values(predicate_list):\n",
    "        \"\"\"Helper to get all values from a list of predicates\"\"\"\n",
    "        values = []\n",
    "        for pred in predicate_list:\n",
    "            for obj in g.objects(ontology_uri, pred):\n",
    "                if isinstance(obj, Literal):\n",
    "                    values.append(str(obj))\n",
    "                else:\n",
    "                    values.append(str(obj))\n",
    "        return values if values else None\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = {\n",
    "        'ontology_uri': str(ontology_uri),\n",
    "        'title': get_value([DCTERMS.title, DC.title, RDFS.label]),\n",
    "        'description': get_value([DCTERMS.description, DC.description, RDFS.comment]),\n",
    "        'created': get_value([DCTERMS.created]),\n",
    "        'issued': get_value([DCTERMS.issued]),\n",
    "        'modified': get_value([DCTERMS.modified]),\n",
    "        'creator': get_all_values([DCTERMS.creator, DC.creator]),\n",
    "        'license': get_value([DCTERMS.license, DC.rights]),\n",
    "        'publisher': get_value([DCTERMS.publisher, DC.publisher]),\n",
    "        'version': get_value([OWL.versionInfo, DCTERMS.hasVersion]),\n",
    "        'preferred_prefix': get_value([VANN.preferredNamespacePrefix]),\n",
    "        'classes_count': sum(1 for _ in g.subjects(RDF.type, OWL.Class)),\n",
    "        'data_properties_count': sum(1 for _ in g.subjects(RDF.type, OWL.DatatypeProperty)),\n",
    "        'object_properties_count': sum(1 for _ in g.subjects(RDF.type, OWL.ObjectProperty))\n",
    "    }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def build_uri_prefix_mapping(ontologies: List[Dict]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build mapping of normalized URIs to their BE-OLS prefix.\n",
    "    \n",
    "    Args:\n",
    "        ontologies: List of ontology dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping normalized URIs to prefixes\n",
    "    \"\"\"\n",
    "    uri_to_prefix: Dict[str, str] = {}\n",
    "    for onto in ontologies:\n",
    "        normalized = normalize_uri(onto['uri'])\n",
    "        uri_to_prefix[normalized] = onto['prefix_manual']\n",
    "    return uri_to_prefix\n",
    "\n",
    "\n",
    "def build_extended_uri_prefix_mapping(ontologies: List[Dict]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a complete mapping of normalized URIs to their most common prefix.\n",
    "    \n",
    "    Args:\n",
    "        ontologies: List of ontology dictionaries with 'prefix', 'uri', \n",
    "                   and 'referenced_ontologies' keys\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping normalized URIs to their most common prefix\n",
    "    \"\"\"\n",
    "    uri_prefix_counts: Dict[str, Counter] = {}\n",
    "    \n",
    "    for onto in ontologies:\n",
    "        if onto.get('prefix_auto') and onto.get('uri'):\n",
    "            prefix = onto['prefix_auto']\n",
    "            uri = onto['uri']\n",
    "            normalized = normalize_uri(uri)\n",
    "            \n",
    "            if normalized not in uri_prefix_counts:\n",
    "                uri_prefix_counts[normalized] = Counter()\n",
    "            if prefix is not None:\n",
    "                uri_prefix_counts[normalized][prefix] += 1\n",
    "        \n",
    "        ref_list = onto.get('referenced_ontologies', [])\n",
    "        if ref_list:\n",
    "            for ref in ref_list:\n",
    "                prefix = ref.get('prefix_auto')\n",
    "                uri = ref.get('ontology_base', '')\n",
    "                \n",
    "                if uri:\n",
    "                    normalized = normalize_uri(uri)\n",
    "                    \n",
    "                    if normalized not in uri_prefix_counts:\n",
    "                        uri_prefix_counts[normalized] = Counter()\n",
    "                    if prefix is not None:\n",
    "                        uri_prefix_counts[normalized][prefix] += 1\n",
    "    \n",
    "    uri_to_prefix: Dict[str, str] = {}\n",
    "    for normalized_uri, prefix_counter in uri_prefix_counts.items():\n",
    "        if prefix_counter:\n",
    "            most_common_prefix = prefix_counter.most_common(1)[0][0]\n",
    "            uri_to_prefix[normalized_uri] = most_common_prefix\n",
    "    \n",
    "    return uri_to_prefix\n",
    "\n",
    "\n",
    "def fix_referenced_ontology_prefixes(onto: Dict, uri_to_prefix: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Fix None prefix values in referenced ontologies using known mappings.\n",
    "    \"\"\"\n",
    "    ref_list = onto.get('referenced_ontologies', [])\n",
    "    if ref_list:\n",
    "        for ref in ref_list:\n",
    "            if ref.get('prefix_auto') is None:\n",
    "                uri = ref.get('ontology_base', '')\n",
    "                if uri:\n",
    "                    normalized = normalize_uri(uri)\n",
    "                    found_prefix = uri_to_prefix.get(normalized)\n",
    "                    if found_prefix:\n",
    "                        ref['prefix_auto'] = found_prefix\n",
    "\n",
    "\n",
    "def create_linked_ontology_lists(onto, uri_to_prefix, debug=False):\n",
    "    \"\"\"\n",
    "    Create lists of linked AECO and upper ontologies.\n",
    "    \n",
    "    Args:\n",
    "        onto: Ontology dictionary\n",
    "        uri_to_prefix: Mapping of normalized URIs to BE-OLS prefixes\n",
    "        debug: If True, print debug information\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (linked_aeco, linked_upper) lists\n",
    "    \"\"\"\n",
    "    excluded_prefixes = {'owl', 'rdf', 'xml', 'xsd', 'rdfs'}\n",
    "\n",
    "    linked_aeco = []\n",
    "    linked_upper = []\n",
    "\n",
    "    # Normalize self URI for comparison\n",
    "    normalized_self_uri = normalize_uri(onto['uri'])\n",
    "\n",
    "    if debug:\n",
    "        print(f\"  Processing {onto['prefix_manual']}:\")\n",
    "        print(f\"    Self URI (normalized): {normalized_self_uri}\")\n",
    "\n",
    "    for ref in onto.get('referenced_ontologies', []):\n",
    "        # Normalize the referenced ontology base for comparison\n",
    "        ontology_base = normalize_uri(ref['ontology_base'])\n",
    "\n",
    "        # Skip self-references\n",
    "        if ontology_base == normalized_self_uri:\n",
    "            if debug:\n",
    "                print(f\"    Skipping self-reference: {ontology_base}\")\n",
    "            continue\n",
    "\n",
    "        # Check if this is an AECO ontology (exists in our BE-OLS list)\n",
    "        if ontology_base in uri_to_prefix:\n",
    "            aeco_prefix = uri_to_prefix[ontology_base]\n",
    "            linked_aeco.append(aeco_prefix)\n",
    "            if debug:\n",
    "                print(f\"    AECO: {ref.get('prefix', '(no prefix)')} -> {aeco_prefix} (matched {ontology_base})\")\n",
    "        else:\n",
    "            # It's an upper/external ontology\n",
    "            if ref['prefix_auto'] and ref['prefix_auto'] not in excluded_prefixes:\n",
    "                linked_upper.append(ref['prefix_auto'])\n",
    "                if debug:\n",
    "                    print(f\"    UPPER: {ref['prefix_auto']} ({ontology_base} not in BE-OLS)\")\n",
    "            elif ref['prefix_auto'] is None:\n",
    "                if debug:\n",
    "                    print(f\"    EXTERNAL (no prefix, not in BE-OLS): {ontology_base}\")\n",
    "            elif debug:\n",
    "                print(f\"    Excluded: {ref['prefix_auto']} ({ontology_base})\")\n",
    "    \n",
    "    return linked_aeco, linked_upper\n",
    "\n",
    "\n",
    "def compare_ontology_lists(list_a, list_b):\n",
    "    \"\"\"Compare two ontology lists and print differences.\"\"\"\n",
    "    set_a = set(list_a) if list_a else set()\n",
    "    if list_b:\n",
    "        set_b = set(item.strip() for item in list_b.split(\",\"))\n",
    "    else:\n",
    "        set_b = set()\n",
    "\n",
    "    in_a_not_b = set_a - set_b\n",
    "    in_b_not_a = set_b - set_a\n",
    "\n",
    "    print(\"  In Auto but not BE-OLS:\", in_a_not_b)\n",
    "    print(\"  In BE-OLS but not Auto:\", in_b_not_a)\n",
    "\n",
    "\n",
    "def compare_ontology_strings(string_a: str, string_b: str):\n",
    "    \"\"\"Compare two strings and print the difference.\"\"\"\n",
    "    diff_str = \"\"\n",
    "    if string_a is None:\n",
    "        if string_b is None:\n",
    "            diff_str = \"  Same (None)\"\n",
    "        else:\n",
    "            diff_str = \"  In BE-OLS but not Auto: \" + str(string_b)\n",
    "    else:\n",
    "        if string_b is None:\n",
    "            diff_str = \"  In Auto but not BE-OLS: \" + str(string_a)\n",
    "        else:\n",
    "            if str(string_a) == str(string_b):\n",
    "                diff_str = \"  Same\"\n",
    "            else:\n",
    "                diff_str = \"  Different: \" + str(string_a) + \" | \" + str(string_b)\n",
    "\n",
    "    print(diff_str)\n",
    "\n",
    "\n",
    "def compare_auto_and_manual(onto):\n",
    "    \"\"\"Compare auto-extracted data against BE-OLS data and print differences.\"\"\"\n",
    "    print(onto['prefix_manual'], \":\")\n",
    "\n",
    "    if not onto['ttl_exists']:\n",
    "        print(\" No Auto data (no ttl file).\")\n",
    "    else:\n",
    "        print(\" Comparison prefix:\")\n",
    "        compare_ontology_strings(onto['prefix_auto'], onto['prefix_manual'])\n",
    "        print(\" Comparison title:\")\n",
    "        compare_ontology_strings(onto['title_auto'], onto['title_manual'])\n",
    "        print(\" Comparison version:\")\n",
    "        compare_ontology_strings(str(onto['version_auto']), str(onto['version_manual']))\n",
    "        print(\" Comparison license:\")\n",
    "        compare_ontology_strings(onto['license_auto'], onto['license_manual'])    \n",
    "        print(\" Comparison description:\")\n",
    "        compare_ontology_strings(onto['description_auto'], onto['description_manual'])\n",
    "        print(\" Comparison linked_aeco:\")\n",
    "        compare_ontology_lists(onto['linked_aeco_auto'], onto['linked_aeco_manual'])\n",
    "        print(\" Comparison linked_upper:\")\n",
    "        compare_ontology_lists(onto['linked_upper_auto'], onto['linked_upper_manual'])\n",
    "\n",
    "\n",
    "def create_unified_columns(onto):\n",
    "    \"\"\"Placeholder for creating unified columns from auto and BE-OLS data.\"\"\"\n",
    "    return None\n",
    "\n",
    "def create_final_fields(onto):\n",
    "    \"\"\"\n",
    "    Create '_final' fields by merging auto-extracted and BE-OLS data.\n",
    "    \n",
    "    If a TTL file is available, prefer auto-extracted values.\n",
    "    Otherwise, fall back to BE-OLS values.\n",
    "    \n",
    "    Args:\n",
    "        onto: Ontology dictionary\n",
    "        \n",
    "    Returns:\n",
    "        None (modifies onto in place)\n",
    "    \"\"\"\n",
    "    has_ttl = onto.get('ttl_exists', False)\n",
    "    \n",
    "    if has_ttl:\n",
    "        # TTL file available - prefer auto-extracted values with BE-OLS fallback\n",
    "        onto['prefix_final'] = onto.get('prefix_auto') if onto.get('prefix_auto') else onto.get('prefix_manual')\n",
    "        onto['title_final'] = onto.get('title_auto') if onto.get('title_auto') else onto.get('title_manual')\n",
    "        onto['description_final'] = onto.get('description_manual')  # Always use BE-OLS for description\n",
    "        onto['created_final'] = onto.get('modified_auto') or onto.get('issued_auto') or onto.get('created_auto') or onto.get('created_manual')\n",
    "        onto['license_final'] = onto.get('license_auto') if onto.get('license_auto') else onto.get('license_manual')\n",
    "        onto['version_final'] = onto.get('version_auto') if onto.get('version_auto') else onto.get('version_manual')\n",
    "        onto['linked_aeco_final'] = onto.get('linked_aeco_auto', [])\n",
    "        onto['linked_upper_final'] = onto.get('linked_upper_auto', [])\n",
    "        \n",
    "        # New _final fields for serialization, documentation, annotation\n",
    "        onto['serialization_final'] = onto.get('serialization_auto')\n",
    "        onto['documentation_final'] = onto.get('documentation_manual')\n",
    "        onto['annotation_final'] = onto.get('annotation_auto')\n",
    "        onto['linked_by_final'] = onto.get('linked_by_auto')\n",
    "        onto['linked_by_aeco_final'] = onto.get('linked_by_aeco_auto')\n",
    "\n",
    "        # Fields with only manual values (no auto calculation)\n",
    "        onto['FOOPs_final'] = onto.get('FOOPs_manual')\n",
    "        onto['conforms_to_standards_final'] = onto.get('conforms_to_standards_manual')\n",
    "        onto['conceptual_data_model_final'] = onto.get('conceptual_data_model_manual')\n",
    "        onto['cluster_final'] = onto.get('cluster_manual')\n",
    "        onto['reference_final'] = onto.get('reference_manual')\n",
    "        onto['primary_domain_final'] = onto.get('primary_domain_manual')\n",
    "        onto['secondary_domain_final'] = onto.get('secondary_domain_manual')\n",
    "\n",
    "        # Count fields\n",
    "        onto['classes_count_final'] = onto.get('classes_count_auto')\n",
    "        onto['data_properties_count_final'] = onto.get('data_properties_count_auto')\n",
    "        onto['object_properties_count_final'] = onto.get('object_properties_count_auto')\n",
    "\n",
    "        # Creator and publisher\n",
    "        onto['creator_final'] = onto.get('creator_auto')\n",
    "        onto['publisher_final'] = onto.get('publisher_auto')\n",
    "    else:\n",
    "        # No TTL file - use BE-OLS values\n",
    "        onto['prefix_final'] = onto.get('prefix_manual')\n",
    "        onto['title_final'] = onto.get('title_manual')\n",
    "        onto['description_final'] = onto.get('description_manual')\n",
    "        onto['created_final'] = onto.get('created_manual')\n",
    "        onto['license_final'] = onto.get('license_manual')\n",
    "        onto['version_final'] = onto.get('version_manual')\n",
    "        onto['linked_aeco_final'] = onto.get('linked_aeco_manual', '')\n",
    "        onto['linked_upper_final'] = onto.get('linked_upper_manual', '')\n",
    "        \n",
    "        # New _final fields for serialization, documentation, annotation\n",
    "        onto['serialization_final'] = onto.get('serialization_manual')\n",
    "        onto['documentation_final'] = onto.get('documentation_manual')\n",
    "        onto['annotation_final'] = onto.get('annotation_manual')\n",
    "        onto['linked_by_final'] = onto.get('linked_by_manual')\n",
    "        onto['linked_by_aeco_final'] = onto.get('linked_by_aeco_auto', [])\n",
    "\n",
    "        # Fields with only manual values (no auto calculation)\n",
    "        onto['FOOPs_final'] = onto.get('FOOPs_manual')\n",
    "        onto['conforms_to_standards_final'] = onto.get('conforms_to_standards_manual')\n",
    "        onto['conceptual_data_model_final'] = onto.get('conceptual_data_model_manual')\n",
    "        onto['cluster_final'] = onto.get('cluster_manual')\n",
    "        onto['reference_final'] = onto.get('reference_manual')\n",
    "        onto['primary_domain_final'] = onto.get('primary_domain_manual')\n",
    "        onto['secondary_domain_final'] = onto.get('secondary_domain_manual')\n",
    "\n",
    "        # Count fields\n",
    "        onto['classes_count_final'] = onto.get('classes_count_auto')\n",
    "        onto['data_properties_count_final'] = onto.get('data_properties_count_auto')\n",
    "        onto['object_properties_count_final'] = onto.get('object_properties_count_auto')\n",
    "\n",
    "        # Creator and publisher\n",
    "        onto['creator_final'] = onto.get('creator_auto')\n",
    "        onto['publisher_final'] = onto.get('publisher_auto')\n",
    "    \n",
    "    # Convert empty lists to empty strings\n",
    "    if onto['linked_aeco_final'] == [] or onto['linked_aeco_final'] is None:\n",
    "        onto['linked_aeco_final'] = ''\n",
    "    if onto['linked_upper_final'] == [] or onto['linked_upper_final'] is None:\n",
    "        onto['linked_upper_final'] = ''\n",
    "    \n",
    "    # Convert None to empty string for documentation (since no auto value is calculated)\n",
    "    if onto.get('documentation_final') is None:\n",
    "        onto['documentation_final'] = ''\n",
    "\n",
    "    # Normalize yes/no to TRUE/FALSE for consistency\n",
    "    for field in ['serialization_final', 'documentation_final', 'annotation_final']:\n",
    "        val = onto.get(field, '')\n",
    "        if val and str(val).lower() in ['yes', 'true']:\n",
    "            onto[field] = True\n",
    "        elif val and str(val).lower() in ['no', 'false']:\n",
    "            onto[field] = False\n",
    "\n",
    "\n",
    "def calculate_annotation_coverage(ttl_file_path):\n",
    "    \"\"\"\n",
    "    Analyze annotation coverage for Classes, Object Properties, Data Properties, and Annotation Properties.\n",
    "    \n",
    "    Checks whether defined elements have rdfs:comment or rdfs:label annotations.\n",
    "    \n",
    "    Args:\n",
    "        ttl_file_path: Path to the TTL file\n",
    "        \n",
    "    Returns:\n",
    "        dict with:\n",
    "            - total_elements: total count of Classes + ObjectProperties + DataProperties + AnnotationProperties\n",
    "            - annotated_elements: count of elements with at least one annotation\n",
    "            - coverage_percent: percentage of annotated elements\n",
    "            - has_annotations: 'yes' if coverage >= 50%, 'no' otherwise\n",
    "    \"\"\"\n",
    "    g = Graph()\n",
    "    g.parse(ttl_file_path, format='turtle')\n",
    "    \n",
    "    # Find the ontology's own namespace\n",
    "    own_namespaces = set()\n",
    "    for s in g.subjects(RDF.type, OWL.Ontology):\n",
    "        uri = str(s)\n",
    "        if uri.startswith('http://') or uri.startswith('https://'):\n",
    "            if '#' in uri:\n",
    "                own_namespaces.add(uri.rsplit('#', 1)[0] + '#')\n",
    "            else:\n",
    "                own_namespaces.add(uri.rsplit('/', 1)[0] + '/')\n",
    "    \n",
    "    # Also check @base and default prefix\n",
    "    for prefix, ns in g.namespaces():\n",
    "        ns_str = str(ns)\n",
    "        if prefix == '' or prefix is None:\n",
    "            own_namespaces.add(ns_str)\n",
    "    \n",
    "    def is_own_element(uri):\n",
    "        \"\"\"Check if URI belongs to this ontology\"\"\"\n",
    "        uri_str = str(uri)\n",
    "        for ns in own_namespaces:\n",
    "            if uri_str.startswith(ns):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def has_annotation(subject):\n",
    "        \"\"\"Check if subject has rdfs:comment\"\"\"\n",
    "        for _ in g.objects(subject, RDFS.comment):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # Collect all Classes, ObjectProperties, DataProperties defined in this ontology\n",
    "    elements = []\n",
    "    \n",
    "    # Classes\n",
    "    for s in g.subjects(RDF.type, OWL.Class):\n",
    "        if is_own_element(s):\n",
    "            elements.append(s)\n",
    "    \n",
    "    # Object Properties\n",
    "    for s in g.subjects(RDF.type, OWL.ObjectProperty):\n",
    "        if is_own_element(s):\n",
    "            elements.append(s)\n",
    "    \n",
    "    # Data Properties\n",
    "    for s in g.subjects(RDF.type, OWL.DatatypeProperty):\n",
    "        if is_own_element(s):\n",
    "            elements.append(s)\n",
    "    \n",
    "    # Annotation Properties\n",
    "    for s in g.subjects(RDF.type, OWL.AnnotationProperty):\n",
    "        if is_own_element(s):\n",
    "            elements.append(s)\n",
    "    \n",
    "    total = len(elements)\n",
    "    annotated = sum(1 for e in elements if has_annotation(e))\n",
    "    \n",
    "    coverage = (annotated / total * 100) if total > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_elements': total,\n",
    "        'annotated_elements': annotated,\n",
    "        'coverage_percent': round(coverage, 1),\n",
    "        'has_annotations': True if coverage >= 50 else False\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_linked_by(ontologies):\n",
    "    \"\"\"\n",
    "    Calculate the 'linked_by' field for each ontology.\n",
    "    \n",
    "    An ontology is marked as linked_by (TRUE) if any other ontology in the BE-OLS\n",
    "    list references it in their linked_aeco or linked_upper lists.\n",
    "    \n",
    "    Args:\n",
    "        ontologies: List of ontology dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        None (modifies ontologies in place)\n",
    "    \"\"\"\n",
    "    # Build a set of all prefixes\n",
    "    all_prefixes = set()\n",
    "    for onto in ontologies:\n",
    "        prefix = onto.get('prefix_manual') or onto.get('prefix_auto')\n",
    "        if prefix:\n",
    "            all_prefixes.add(prefix.lower())\n",
    "    \n",
    "    # For each ontology, check if it's referenced by others\n",
    "    for onto in ontologies:\n",
    "        onto_prefix = onto.get('prefix_manual') or onto.get('prefix_auto')\n",
    "        if not onto_prefix:\n",
    "            onto['linked_by_auto'] = False\n",
    "            continue\n",
    "        \n",
    "        onto_prefix_lower = onto_prefix.lower()\n",
    "        is_reused = False\n",
    "        \n",
    "        # Check all other ontologies\n",
    "        for other in ontologies:\n",
    "            if other is onto:\n",
    "                continue\n",
    "            \n",
    "            # Check linked_aeco (both auto-extracted and BE-OLS)\n",
    "            linked_aeco = other.get('linked_aeco', [])\n",
    "            if isinstance(linked_aeco, list):\n",
    "                if onto_prefix_lower in [p.lower() for p in linked_aeco if p]:\n",
    "                    is_reused = True\n",
    "                    break\n",
    "            \n",
    "            linked_aeco_beols = other.get('linked_aeco_manual', '')\n",
    "            if isinstance(linked_aeco_beols, str) and linked_aeco_beols:\n",
    "                aeco_list = [p.strip().lower() for p in linked_aeco_beols.split(',')]\n",
    "                if onto_prefix_lower in aeco_list:\n",
    "                    is_reused = True\n",
    "                    break\n",
    "            \n",
    "            # Check linked_upper (both auto-extracted and BE-OLS)\n",
    "            linked_upper = other.get('linked_upper', [])\n",
    "            if isinstance(linked_upper, list):\n",
    "                if onto_prefix_lower in [p.lower() for p in linked_upper if p]:\n",
    "                    is_reused = True\n",
    "                    break\n",
    "            \n",
    "            linked_upper_beols = other.get('linked_upper_manual', '')\n",
    "            if isinstance(linked_upper_beols, str) and linked_upper_beols:\n",
    "                upper_list = [p.strip().lower() for p in linked_upper_beols.split(',')]\n",
    "                if onto_prefix_lower in upper_list:\n",
    "                    is_reused = True\n",
    "                    break\n",
    "        \n",
    "        onto['linked_by_auto'] = True if is_reused else False\n",
    "\n",
    "        # Build list of AECO ontologies that link to this one\n",
    "        linked_by_aeco = []\n",
    "        for other in ontologies:\n",
    "            if other is onto:\n",
    "                continue\n",
    "            other_prefix = other.get('prefix_manual') or other.get('prefix_auto')\n",
    "            if not other_prefix:\n",
    "                continue\n",
    "            # Check if other's linked_aeco_auto contains this ontology's prefix\n",
    "            other_linked_aeco = other.get('linked_aeco_auto', [])\n",
    "            if isinstance(other_linked_aeco, list):\n",
    "                if onto_prefix_lower in [p.lower() for p in other_linked_aeco if p]:\n",
    "                    linked_by_aeco.append(other_prefix)\n",
    "        onto['linked_by_aeco_auto'] = linked_by_aeco\n",
    "\n",
    "\n",
    "def calculate_scores(onto):\n",
    "    \"\"\"\n",
    "    Calculate alignment, accessibility, and quality scores for an ontology.\n",
    "    \n",
    "    Outputs 6 fields:\n",
    "    - linkage_to_upper: 'yes'/'no' - whether linked to upper ontologies\n",
    "    - linkage_to_aeco: 'yes'/'no' - whether linked to AECO ontologies\n",
    "    - linkage_to_aeco_meta: 'yes'/'no' - whether linked to meta schema ontologies (bot, brick, ifc4-add2)\n",
    "    - score_alignment: 0-3 sum of above\n",
    "    - score_accessibility: 0-3 based on data model, serialization, URI\n",
    "    - score_quality: 0-2 based on documentation, annotations\n",
    "    \n",
    "    Args:\n",
    "        onto: Ontology dictionary\n",
    "        \n",
    "    Returns:\n",
    "        None (modifies onto in place)\n",
    "    \"\"\"\n",
    "    meta_list = ['bot', 'brick', 'ifc4-add2']\n",
    "    \n",
    "    # Alignment Score - use _final fields\n",
    "    alignment = [0, 0, 0]\n",
    "    \n",
    "    # Check linked_upper_final\n",
    "    linked_upper = onto.get('linked_upper_final', '')\n",
    "    if linked_upper and linked_upper != 'n/a':\n",
    "        if isinstance(linked_upper, list):\n",
    "            if len(linked_upper) > 0:\n",
    "                alignment[0] = 1\n",
    "        elif isinstance(linked_upper, str) and linked_upper.strip():\n",
    "            alignment[0] = 1\n",
    "    \n",
    "    # Check linked_aeco_final\n",
    "    linked_aeco = onto.get('linked_aeco_final', '')\n",
    "    if linked_aeco and linked_aeco != 'n/a':\n",
    "        aeco_list = []\n",
    "        if isinstance(linked_aeco, list):\n",
    "            aeco_list = linked_aeco\n",
    "        elif isinstance(linked_aeco, str) and linked_aeco.strip():\n",
    "            aeco_list = [s.strip() for s in linked_aeco.split(',')]\n",
    "        \n",
    "        if len(aeco_list) > 0:\n",
    "            alignment[1] = 1\n",
    "            if any(s in aeco_list for s in meta_list):\n",
    "                alignment[2] = 1\n",
    "    \n",
    "    onto['linkage_to_upper'] = True if alignment[0] == 1 else False\n",
    "    onto['linkage_to_aeco'] = True if alignment[1] == 1 else False\n",
    "    onto['linkage_to_aeco_meta'] = True if alignment[2] == 1 else False\n",
    "    onto['score_alignment'] = sum(alignment)\n",
    "    \n",
    "    # Accessibility Score\n",
    "    accessibility = [0, 0, 0]\n",
    "    \n",
    "    data_model = onto.get('conceptual_data_model_manual', '')\n",
    "    if data_model and str(data_model).lower() in ['yes', 'true']:\n",
    "        accessibility[0] = 1\n",
    "    \n",
    "    serialization = onto.get('serialization_final', '')\n",
    "    if serialization and serialization == True or str(serialization).lower() in ['yes', 'true']:\n",
    "        accessibility[1] = 1\n",
    "    \n",
    "    uri = onto.get('uri', '')\n",
    "    if uri and uri != 'n/a' and uri != '':\n",
    "        accessibility[2] = 1\n",
    "    \n",
    "    onto['score_accessibility'] = sum(accessibility)\n",
    "    \n",
    "    # Quality Score (0-2)\n",
    "    quality = [0, 0]\n",
    "    \n",
    "    documentation = onto.get('documentation_final', '')\n",
    "    if documentation and documentation == True or str(documentation).lower() in ['yes', 'true']:\n",
    "        quality[0] = 1\n",
    "    \n",
    "    #annotation = onto.get('annotation_final', '')\n",
    "    #if annotation and annotation == True or str(annotation).lower() in ['yes', 'true']:\n",
    "    #    quality[1] = 1\n",
    "    annotation_coverage = onto.get('annotation_coverage_percent', '')\n",
    "    if annotation_coverage:\n",
    "        quality[1] = round(annotation_coverage / 100.0,1)\n",
    "    \n",
    "    onto['score_quality'] = sum(quality)\n",
    "\n",
    "\n",
    "\n",
    "def process_ontologies(excel_file, ttl_folder, debug=False):\n",
    "    \"\"\"\n",
    "    Process all ontologies listed in the Excel file.\n",
    "    \n",
    "    Args:\n",
    "        excel_file: Path to Excel file with ontology list\n",
    "        ttl_folder: Path to folder containing TTL files\n",
    "        debug: If True, print debug information for prefix extraction\n",
    "        \n",
    "    Returns:\n",
    "        ontologies: the list of ontologies from excel_file augmented with new data\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(excel_file)\n",
    "    \n",
    "    ontologies = []\n",
    "    ttl_folder_path = Path(ttl_folder)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        prefix_manual = row['prefix'] if pd.notna(row['prefix']) else \"no_prefix\"\n",
    "        uri = row['uri'] if pd.notna(row['uri']) else \"\"\n",
    "        \n",
    "        title_manual = row['title'] if 'title' in row and pd.notna(row['title']) else None\n",
    "        cluster_manual = row['cluster'] if 'cluster' in row and pd.notna(row['cluster']) else None\n",
    "        reference_manual = row['reference'] if 'reference' in row and pd.notna(row['reference']) else None\n",
    "        primary_domain_manual = row['primary_domain'] if 'primary_domain' in row and pd.notna(row['primary_domain']) else None\n",
    "        secondary_domain_manual = row['secondary_domain'] if 'secondary_domain' in row and pd.notna(row['secondary_domain']) else None\n",
    "        conceptual_data_model_manual = row['conceptual_data_model'] if 'conceptual_data_model' in row and pd.notna(row['conceptual_data_model']) else None\n",
    "        serialization_manual = row['serialization'] if 'serialization' in row and pd.notna(row['serialization']) else None\n",
    "        documentation_manual = row['documentation'] if 'documentation' in row and pd.notna(row['documentation']) else None\n",
    "        annotation_manual = row['annotation'] if 'annotation' in row and pd.notna(row['annotation']) else None\n",
    "        linked_by_manual = row['linked_by'] if 'linked_by' in row and pd.notna(row['linked_by']) else None\n",
    "        FOOPs_manual = row['FOOPs'] if 'FOOPs' in row and pd.notna(row['FOOPs']) else None\n",
    "        conforms_to_standards_manual = row['conforms_to_standards'] if 'conforms_to_standards' in row and pd.notna(row['conforms_to_standards']) else None\n",
    "        \n",
    "        version_manual = row['version'] if 'version' in row and pd.notna(row['version']) else None\n",
    "        created_manual = row['created'] if 'created' in row and pd.notna(row['created']) else None\n",
    "        license_manual = row['license'] if 'license' in row and pd.notna(row['license']) else None\n",
    "        description_manual = row['description'] if 'description' in row and pd.notna(row['description']) else None\n",
    "        linked_aeco_manual = row['linked_aeco'] if 'linked_aeco' in row and pd.notna(row['linked_aeco']) else None\n",
    "        linked_upper_manual = row['linked_upper'] if 'linked_upper' in row and pd.notna(row['linked_upper']) else None\n",
    "        \n",
    "        result = {\n",
    "            'uri': uri,\n",
    "            'ttl_exists': False,\n",
    "            'error': '',\n",
    "            'prefix_manual': prefix_manual,\n",
    "            'prefix_auto': None,\n",
    "            'prefix_final': None,\n",
    "            'title_manual': title_manual,\n",
    "            'title_auto': None,\n",
    "            'title_final': None,\n",
    "            'description_manual': description_manual,\n",
    "            'description_auto': None,\n",
    "            'description_final': None,\n",
    "            'created_manual': created_manual,\n",
    "            'created_auto': None,\n",
    "            'issued_auto': None,\n",
    "            'modified_auto': None,\n",
    "            'created_final': None,\n",
    "            'license_manual': license_manual,\n",
    "            'license_auto': None,\n",
    "            'license_final': None,\n",
    "            'version_manual': version_manual,\n",
    "            'version_auto': None,\n",
    "            'version_final': None,\n",
    "            'linked_aeco_manual': linked_aeco_manual,\n",
    "            'linked_aeco_auto': [],\n",
    "            'linked_aeco_final': None,\n",
    "            'linked_upper_manual': linked_upper_manual,\n",
    "            'linked_upper_auto': [],\n",
    "            'linked_upper_final': None,\n",
    "            'serialization_manual': serialization_manual,\n",
    "            'serialization_auto': False,\n",
    "            'serialization_final': None,\n",
    "            'documentation_manual': documentation_manual,\n",
    "            'documentation_auto': None,\n",
    "            'documentation_final': None,\n",
    "            'annotation_manual': annotation_manual,\n",
    "            'annotation_auto': None,\n",
    "            'annotation_final': None,\n",
    "            'annotation_coverage_percent': None,\n",
    "            'linked_by_manual': linked_by_manual,\n",
    "            'linked_by_auto': None,\n",
    "            'linked_by_final': None,\n",
    "            'linked_by_aeco_auto': [],\n",
    "            'linked_by_aeco_final': None,\n",
    "            'FOOPs_manual': FOOPs_manual,\n",
    "            'FOOPs_auto': None,\n",
    "            'FOOPs_final': None,\n",
    "            'conforms_to_standards_manual': conforms_to_standards_manual,\n",
    "            'conforms_to_standards_auto': None,\n",
    "            'conforms_to_standards_final': None,\n",
    "            'conceptual_data_model_manual': conceptual_data_model_manual,\n",
    "            'conceptual_data_model_auto': None,\n",
    "            'conceptual_data_model_final': None,\n",
    "            'cluster_manual': cluster_manual,\n",
    "            'cluster_auto': None,\n",
    "            'cluster_final': None,\n",
    "            'reference_manual': reference_manual,\n",
    "            'reference_auto': None,\n",
    "            'reference_final': None,\n",
    "            'primary_domain_manual': primary_domain_manual,\n",
    "            'primary_domain_auto': None,\n",
    "            'primary_domain_final': None,\n",
    "            'secondary_domain_manual': secondary_domain_manual,\n",
    "            'secondary_domain_auto': None,\n",
    "            'secondary_domain_final': None,\n",
    "            'creator_auto': [],\n",
    "            'creator_final': None,\n",
    "            'publisher_auto': None,\n",
    "            'publisher_final': None,\n",
    "            'referenced_ontologies': [],\n",
    "            'classes_count_auto': None,\n",
    "            'classes_count_final': None,\n",
    "            'data_properties_count_auto': None,\n",
    "            'data_properties_count_final': None,\n",
    "            'object_properties_count_auto': None,\n",
    "            'object_properties_count_final': None,\n",
    "        }\n",
    "\n",
    "        # Normalize yes/no to boolean for _manual fields\n",
    "        for field in ['conceptual_data_model_manual', 'serialization_manual', 'documentation_manual', 'annotation_manual', 'linked_by_manual']:\n",
    "            val = result.get(field)\n",
    "            if val is not None:\n",
    "                if str(val).lower() in ['yes', 'true']:\n",
    "                    result[field] = True\n",
    "                elif str(val).lower() in ['no', 'false']:\n",
    "                    result[field] = False\n",
    "        \n",
    "        ttl_file = ttl_folder_path / f\"{prefix_manual}.ttl\"\n",
    "\n",
    "        if ttl_file.exists():\n",
    "            result['ttl_exists'] = True\n",
    "            \n",
    "            print(f\"\\nProcessing {prefix_manual}.ttl...\")\n",
    "            try:\n",
    "                referenced_ontologies = extract_ontology_uris(ttl_file, debug=debug)\n",
    "                metadata = extract_ontology_metadata(ttl_file)\n",
    "\n",
    "                print(f\" Total referenced ontologies found: {len(referenced_ontologies)}\")\n",
    "                if debug:\n",
    "                    for ref in referenced_ontologies:\n",
    "                        print(f\"   - {ref['prefix_auto']}: {ref['ontology_base']}\")\n",
    "                    \n",
    "                result['prefix_auto'] = metadata['preferred_prefix']\n",
    "                result['title_auto'] = metadata['title']\n",
    "                result['version_auto'] = metadata['version']\n",
    "                result['created_auto'] = metadata['created']\n",
    "                result['issued_auto'] = metadata['issued']\n",
    "                result['modified_auto'] = metadata['modified']\n",
    "                result['creator_auto'] = metadata['creator']\n",
    "                result['license_auto'] = metadata['license']\n",
    "                result['description_auto'] = metadata['description']\n",
    "                result['publisher_auto'] = metadata['publisher']\n",
    "\n",
    "                result['classes_count_auto'] = metadata['classes_count']\n",
    "                result['data_properties_count_auto'] = metadata['data_properties_count']\n",
    "                result['object_properties_count_auto'] = metadata['object_properties_count']\n",
    "\n",
    "                # Extract annotation coverage\n",
    "                annotation_info = calculate_annotation_coverage(ttl_file)\n",
    "                result['annotation_auto'] = annotation_info['has_annotations']\n",
    "                result['annotation_coverage_percent'] = annotation_info['coverage_percent']\n",
    "                \n",
    "                # Set serialization based on TTL existence\n",
    "                result['serialization_auto'] = True\n",
    "                result['referenced_ontologies'] = referenced_ontologies\n",
    "                ontologies.append(result)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {prefix_manual}.ttl: {e}\")\n",
    "                result['error'] = str(e)\n",
    "                ontologies.append(result)\n",
    "        else:\n",
    "            print(f\"Warning: {ttl_file} not found\")\n",
    "            result['error'] = 'File not found'\n",
    "            ontologies.append(result) \n",
    "\n",
    "    # Build URI to prefix mappings\n",
    "    uri_to_prefix = build_uri_prefix_mapping(ontologies)\n",
    "    extended_uri_to_prefix = build_extended_uri_prefix_mapping(ontologies)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nURI to prefix mapping (from BE-OLS):\")\n",
    "        for uri, prefix in sorted(uri_to_prefix.items()):\n",
    "            print(f\"  {uri} -> {prefix}\")\n",
    "\n",
    "    # Fix the missing prefixes\n",
    "    for onto in ontologies:\n",
    "        fix_referenced_ontology_prefixes(onto, extended_uri_to_prefix)\n",
    "\n",
    "    # Create linked_aeco and linked_upper lists\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Creating linked ontology lists...\")\n",
    "    print(\"=\"*60)\n",
    "    for onto in ontologies:\n",
    "        linked_aeco, linked_upper = create_linked_ontology_lists(onto, uri_to_prefix, debug=debug)        \n",
    "        onto['linked_aeco_auto'] = linked_aeco\n",
    "        onto['linked_upper_auto'] = linked_upper\n",
    "\n",
    "    # Compare auto-extracted vs BE-OLS data\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON: Auto-extracted vs BE-OLS data\")\n",
    "    print(\"=\"*60)\n",
    "    for onto in ontologies:\n",
    "        compare_auto_and_manual(onto)\n",
    "    \n",
    "    # Create unified columns\n",
    "    for onto in ontologies:\n",
    "        create_unified_columns(onto)\n",
    "\n",
    "    # Calculate linked_by field (must be done before create_final_fields)\n",
    "    calculate_linked_by(ontologies)\n",
    "\n",
    "    # Create final fields by merging auto-extracted and BE-OLS data\n",
    "    for onto in ontologies:\n",
    "        create_final_fields(onto)\n",
    "\n",
    "\n",
    "    # Calculate alignment, accessibility, and quality scores\n",
    "    for onto in ontologies:\n",
    "        calculate_scores(onto)\n",
    "\n",
    "    return ontologies\n",
    "\n",
    "\n",
    "\n",
    "def write_output_JSON(ontologies, output_json):\n",
    "    \"\"\"Write ontologies data to JSON file.\"\"\"\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ontologies, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nJSON processing complete. Results saved to {output_json}\")\n",
    "\n",
    "def write_output_EXCEL(ontologies, output_excel):\n",
    "    \"\"\"Write ontologies data to Excel file.\"\"\"\n",
    "    excel_data = []\n",
    "    for result in ontologies:\n",
    "        excel_row = result.copy()\n",
    "        \n",
    "        if 'creator_auto' in excel_row and isinstance(excel_row['creator_auto'], list):\n",
    "            excel_row['creator_auto'] = ', '.join(excel_row['creator_auto']) if excel_row['creator_auto'] else ''\n",
    "        \n",
    "        if 'referenced_ontologies' in excel_row and isinstance(excel_row['referenced_ontologies'], list):\n",
    "            formatted_refs = []\n",
    "            for ref in excel_row['referenced_ontologies']:\n",
    "                if isinstance(ref, dict):\n",
    "                    prefix_val = ref.get('prefix_auto', 'None')\n",
    "                    uri_val = ref.get('ontology_base', '')\n",
    "                    formatted_refs.append(f\"{{{prefix_val}: {uri_val}}}\")\n",
    "                else:\n",
    "                    formatted_refs.append(str(ref))\n",
    "            excel_row['referenced_ontologies'] = ', '.join(formatted_refs)\n",
    "        \n",
    "        if 'linked_aeco_auto' in excel_row and isinstance(excel_row['linked_aeco_auto'], list) and excel_row['linked_aeco_auto']:\n",
    "            excel_row['linked_aeco_auto'] = ', '.join(str(n) for n in excel_row['linked_aeco_auto'] if n is not None)\n",
    "        else:\n",
    "            excel_row['linked_aeco_auto'] = ''\n",
    "\n",
    "        if 'linked_upper_auto' in excel_row and isinstance(excel_row['linked_upper_auto'], list) and excel_row['linked_upper_auto']:\n",
    "            excel_row['linked_upper_auto'] = ', '.join(str(n) for n in excel_row['linked_upper_auto'] if n is not None)\n",
    "        else:\n",
    "            excel_row['linked_upper_auto'] = ''\n",
    "\n",
    "        # Handle _final list fields\n",
    "        if 'linked_aeco_final' in excel_row and isinstance(excel_row['linked_aeco_final'], list):\n",
    "            excel_row['linked_aeco_final'] = ', '.join(str(n) for n in excel_row['linked_aeco_final'] if n is not None) if excel_row['linked_aeco_final'] else ''\n",
    "        if 'linked_by_aeco_auto' in excel_row and isinstance(excel_row['linked_by_aeco_auto'], list) and excel_row['linked_by_aeco_auto']:\n",
    "            excel_row['linked_by_aeco_auto'] = ', '.join(str(n) for n in excel_row['linked_by_aeco_auto'] if n is not None)\n",
    "        else:\n",
    "            excel_row['linked_by_aeco_auto'] = ''\n",
    "\n",
    "        if 'linked_by_aeco_final' in excel_row and isinstance(excel_row['linked_by_aeco_final'], list) and excel_row['linked_by_aeco_final']:\n",
    "            excel_row['linked_by_aeco_final'] = ', '.join(str(n) for n in excel_row['linked_by_aeco_final'] if n is not None)\n",
    "        else:\n",
    "            excel_row['linked_by_aeco_final'] = ''\n",
    "\n",
    "        if 'linked_upper_final' in excel_row and isinstance(excel_row['linked_upper_final'], list):\n",
    "            excel_row['linked_upper_final'] = ', '.join(str(n) for n in excel_row['linked_upper_final'] if n is not None) if excel_row['linked_upper_final'] else ''\n",
    "\n",
    "        excel_data.append(excel_row)\n",
    "    \n",
    "    df_output = pd.DataFrame(excel_data)\n",
    "    df_output.to_excel(output_excel, sheet_name='Data', index=False, engine='openpyxl')\n",
    "    \n",
    "    print(f\"Excel processing complete. Results saved to {output_excel}\")\n",
    "    print(f\"Total ontologies processed: {len(ontologies)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths and run processing\n",
    "filepath = Path(file_path_ontologies)\n",
    "ontologies_ttl_folder = filepath.parent / \"Ontologies_TTL\"\n",
    "\n",
    "# Process all ontologies (set debug=True to see detailed prefix/URI matching)\n",
    "ontologies = process_ontologies(file_path_ontologies, ontologies_ttl_folder, debug=False)\n",
    "\n",
    "# Write outputs\n",
    "output_excel = filepath.parent / \"Ontologies_forRepo.xlsx\"\n",
    "write_output_EXCEL(ontologies, output_excel)\n",
    "#output_json = filepath.parent / \"Ontologies_forRepo.json\"\n",
    "#write_output_JSON(ontologies, output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foops_url = \"https://foops.linkeddata.es/assessOntology\"\n",
    "\n",
    "foops_headers = {\n",
    "    \"accept\": \"application/json;charset=UTF-8\",\n",
    "    \"Content-Type\": \"application/json;charset=UTF-8\",\n",
    "}\n",
    "\n",
    "def get_foops_score(uri):\n",
    "    auto_uri = \"\"\n",
    "    auto_title = \"\"\n",
    "    foops_score = -1.0\n",
    "\n",
    "    if uri == 'n/a' or pd.isnull(uri):\n",
    "        foops_score = 0.0\n",
    "    else:\n",
    "        try:\n",
    "            foops_data = {\"ontologyUri\": uri}\n",
    "            response = requests.post(foops_url, headers=foops_headers, json=foops_data)\n",
    "            response = response.json()\n",
    "            print(response)\n",
    "         \n",
    "            if 'ontology_URI' in response:\n",
    "                auto_uri = response['ontology_URI']\n",
    "                auto_title = response['ontology_title']\n",
    "                foops_score = response['overall_score']\n",
    "                #checks = response['checks']\n",
    "            else:\n",
    "                print(\"FOOPs Internal Server Error\")\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(\"HTTP error:\", e.response.status_code, e.response.text)\n",
    "\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"Error: Failed to connect to the server\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"Error: Request timed out\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Unexpected error:\", str(e))\n",
    "\n",
    "    print(f\"  auto_uri: {auto_uri}; auto_title: {auto_title}; foops_score: {foops_score}\")\n",
    "\n",
    "    return foops_score\n",
    "\n",
    "def process_ontologies_foops(ontologies, debug=False):\n",
    "\n",
    "    for index, onto in enumerate(ontologies):\n",
    "        #if index > 5:\n",
    "        #    continue\n",
    "\n",
    "        prefix = onto['prefix_final']\n",
    "        uri = onto['uri']\n",
    "        title = onto['title_final']\n",
    "\n",
    "        print(f\"prefix: {prefix}; title: {title}; uri: {uri}\")\n",
    "        \n",
    "        foops_score = round(get_foops_score(uri),2)\n",
    "        \n",
    "        onto['FOOPs_auto'] = foops_score\n",
    "        if foops_score == -1.0:\n",
    "            onto['FOOPs_auto'] = None\n",
    "            onto['FOOPs_final'] = onto['FOOPs_manual']\n",
    "        else:\n",
    "            onto['FOOPs_auto'] = foops_score\n",
    "            onto['FOOPs_final'] = foops_score\n",
    "    \n",
    "    return ontologies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all ontologies (set debug=True to see detailed prefix/URI matching)\n",
    "ontologies = process_ontologies_foops(ontologies, debug=False)\n",
    "\n",
    "# Write outputs\n",
    "filepath = Path(file_path_ontologies)\n",
    "output_excel = filepath.parent / \"Ontologies_forRepo.xlsx\"\n",
    "write_output_EXCEL(ontologies, output_excel)\n",
    "#output_json = filepath.parent / \"Ontologies_forRepo.json\"\n",
    "#write_output_JSON(ontologies, output_json)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
